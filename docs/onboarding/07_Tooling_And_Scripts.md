# Project Onboarding: Tooling & Scripts

## 8. Scripts

The `convai_narrative_memory_poc/scripts/` directory contains helper shell scripts for managing the Docker environment.

*   `create_topics.sh`
    *   **Purpose**: This is the primary setup script. It initializes the local development environment.
    *   **Actions**:
        1.  Connects to the Kafka container.
        2.  Deletes and recreates all the necessary Kafka topics (`anchors-write`, `recall-request`, etc.). This ensures you start with a clean slate.
        3.  Deletes the Qdrant Docker volume (`qdrant_storage`). This clears all previously stored memories.
    *   **When to use**: Run this script once before you start the Docker Compose stack for the first time, or any time you want to completely reset the system's state.
    *   **Kubernetes Note**: In a production Kubernetes environment, Kafka topics and database schemas would be managed declaratively using Infrastructure as Code (e.g., Terraform) or a dedicated Kubernetes Operator, not manual scripts.

*   `check_stack.sh`
    *   **Purpose**: A simple diagnostic script to verify that the core services are running and accessible.
    *   **Actions**: It checks the health or status of the Kafka and Qdrant containers.
    *   **When to use**: Run this if you're having trouble connecting to Kafka or Qdrant from the worker services.

## 9. Tools

The `convai_narrative_memory_poc/tools/` directory contains Python scripts and applications used for demonstrating, testing, and interacting with the memory system. These are typically run inside the `tools` container via `docker compose run`.

*   `chatbot.py`
    *   **Purpose**: An interactive command-line chatbot interface for the memory system.
    *   **Functionality**: Allows you to have a conversation with the AI. Each of your messages is stored as a memory anchor. The chatbot's responses are generated by combining an LLM-powered persona with the narrative retrieved from the `retell-response` topic.
    *   **Special Commands**:
        *   `/reset_session`: Starts a new "session", preventing memories from the current conversation from being used in recall.
        *   `/advance_time <duration>`: Simulates the passage of time (e.g., `/advance_time 90d`). This is crucial for testing the temporal decay features.
        *   `/reset_time`: Jumps back to the present.
    *   **Kubernetes Note**: This tool is for local development and demonstration. In a production system, a proper API service would publish messages to Kafka, and a frontend application would consume the final `retell-response` topic.

*   `streamlit_app.py`
    *   **Location**: `convai_narrative_memory_poc/ui/streamlit_app.py`
    *   **Purpose**: A rich, web-based graphical user interface for interacting with the system.
    *   **Functionality**: This is the most comprehensive tool for visualization. It provides a chat interface similar to `chatbot.py` but also displays the internal state of the memory system in real-time, including the raw recalled "beats", their activation scores, and the final generated narrative. It makes the abstract concepts of recall and retelling much easier to understand.
    *   **Kubernetes Note**: While Streamlit can be deployed to Kubernetes, in a production scenario this would likely be replaced by a custom-built frontend application that communicates with the memory system's API endpoints.

*   `validation_experiments.py`
    *   **Purpose**: The core script for validating the psychological model.
    *   **Functionality**: This script runs a series of automated experiments. It seeds a predefined set of memories with different ages and salience scores, runs recall queries against them, and tests different values for the decay constant (`λ`). It was used to determine the optimal `λ=0.002`. It can be run as a standalone Python script for rapid, lightweight testing without Docker or Kafka.
    *   **Kubernetes Note**: The logic in this script is perfect for creating a Kubernetes Job that runs as part of a CI/CD pipeline, acting as an integration test to validate that the deployed system is behaving as expected.

*   `demo_three_retells.py`
    *   **Purpose**: A simple, focused script to demonstrate the age-aware narrative generation.
    *   **Functionality**: It seeds three memories (recent, medium-term, old) and then immediately runs a recall query. It prints the raw beats and the final retold narrative, clearly showing how the `Reteller` makes older memories sound "fuzzier".
    *   **Kubernetes Note**: A simple demonstration tool for local use.

*   `seed_and_query.py`
    *   **Purpose**: A basic, end-to-end test of the Kafka pipeline.
    *   **Functionality**: It seeds a single memory and then runs a single query, demonstrating the full message flow from `anchors-write` through to `retell-response`.
    *   **Kubernetes Note**: Like the validation script, this could be adapted into a simple integration test within a CI/CD pipeline.

*   `inspect_qdrant.py`
    *   **Purpose**: A utility to peek inside the vector database.
    *   **Functionality**: It connects to Qdrant and retrieves and prints the most recently stored memory anchors, including their text and metadata. Useful for debugging the `Indexer` service.
    *   **Kubernetes Note**: For a managed database in production, you would use the cloud provider's own UI or CLI tools for inspection. For a self-hosted Qdrant on Kubernetes, you could use `kubectl port-forward` to access its dashboard.
